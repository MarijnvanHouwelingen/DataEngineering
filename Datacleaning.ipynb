{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline 1\n",
    "This pipeline gets data from the first data bucket, processes it through five components: loading the data, cleaning the data, splitting the data, undersampling the train data and uploading it to the cloud. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install (???)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the packages\n",
    "! pip3 install --user --no-cache-dir --upgrade \"kfp>2\" \"google-cloud-pipeline-components>2\" \\\n",
    "                                        google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\"\n",
    "! pip3 freeze | grep aiplatform\n",
    "! python3 -c \"import google_cloud_pipeline_components; print('google_cloud_pipeline_components version: {}'.format(google_cloud_pipeline_components.__version__))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import typing\n",
    "from typing import Dict\n",
    "from typing import NamedTuple\n",
    "from kfp import dsl\n",
    "from kfp.dsl import (Artifact,\n",
    "                        Dataset,\n",
    "                        Input,\n",
    "                        Model,\n",
    "                        Output,\n",
    "                        Metrics,\n",
    "                        ClassificationMetrics,\n",
    "                        component, \n",
    "                        OutputPath, \n",
    "                        InputPath)\n",
    "import google.cloud.aiplatform as aip\n",
    "from google_cloud_pipeline_components.v1.model import ModelUploadOp\n",
    "from google_cloud_pipeline_components.v1.endpoint import (EndpointCreateOp,ModelDeployOp)\n",
    "from google_cloud_pipeline_components.types import artifact_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Google Cloud project that this pipeline runs in.\n",
    "PROJECT_ID = \"assignment-1-399115\"\n",
    "# The region that this pipeline runs in\n",
    "REGION = \"us-central1\"\n",
    "# Specify a Cloud Storage URI that your pipelines service account can access. The artifacts of your pipeline runs are stored within the pipeline root.\n",
    "PIPELINE_ROOT = \"gs://temp_de2023_2056332\"\n",
    "# Specify your assignment\n",
    "ASSIGNMENT = \"Assignment_1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component: Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\"pandas\",\"google-cloud-storage\"],\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def download_data(project_id: str, bucket: str, feature_dataset_name: str, label_dataset_name: str, merged_dataset: Output[Dataset]):\n",
    "    '''Download data'''\n",
    "    from google.cloud import storage\n",
    "    import pandas as pd\n",
    "    import logging \n",
    "    import sys\n",
    "    \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    \n",
    "    # Get the client and bucket\n",
    "    client = storage.Client(project=project_id)\n",
    "    bucket = client.bucket(bucket)\n",
    "\n",
    "     # Download the feature dataset\n",
    "    blob1 = bucket.blob(feature_dataset_name)\n",
    "    blob1.download_to_filename(feature_dataset_name.path + \".csv\")\n",
    "    df_feature = pd.read_csv(feature_dataset_name.path + \".csv\")\n",
    "    \n",
    "    \n",
    "    # Download the label dataset\n",
    "    blob2 = bucket.blob(label_dataset_name)\n",
    "    blob2.download_to_filename(label_dataset_name.path + \".csv\")\n",
    "    df_label = pd.read_csv(feature_dataset_name.path + \".csv\")\n",
    "    \n",
    "    \n",
    "    # Merge the datasets\n",
    "    merged_df = pd.merge(df_feature, df_label, on='Ind_ID')\n",
    "    merged_df.to_csv(merged_dataset.path + \".csv\", index=False, encoding='utf-8-sig')\n",
    "    \n",
    "\n",
    "    logging.info('Download & Merge all Data complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component: cleaning data\n",
    "Irrelevant columns are delted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\"pandas\", \"numpy\"],\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def clean_data(merged_dataset: Input[Dataset], cleaned_dataset: Output[Dataset]):\n",
    "    '''Deletes irrelevant columns and removes NA's'''\n",
    "    import pandas as pd\n",
    "    import logging\n",
    "    import sys\n",
    "    import numpy\n",
    "\n",
    "    # Sets the logging config\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO) \n",
    "\n",
    "    #Loads the merged dataset in\n",
    "    df = pd.read_csv(merged_dataset.path, index_col=None)\n",
    "\n",
    "    # Drops the columns: Birthday_count, Employed_days, Mobile_phone, Work_Phone, Phone, EMAIL_ID, Type_Occupation and Family_Members.\n",
    "    cleandf = df.drop(columns= [\"Birthday_count\", \"Employed_days\", \"Mobile_phone\", \"Work_Phone\", \"Phone\", \"EMAIL_ID\", \"Type_Occupation\", \"Family_Members\"])\n",
    "\n",
    "    # Save the cleaned dataset\n",
    "    cleandf.to_csv(cleaned_dataset.path + \".csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component: Creating Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\"pandas\", \"scikit-learn\"],\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def train_test_split(cleaned_dataset: Input[Dataset], dataset_train: Output[Dataset], dataset_test: Output[Dataset]):\n",
    "    '''train_test_split'''\n",
    "    import pandas as pd\n",
    "    import logging \n",
    "    import sys\n",
    "    from sklearn.model_selection import train_test_split as tts\n",
    "    \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO) \n",
    "    \n",
    "    # get the cleaned data from previous component\n",
    "    alldata = pd.read_csv(cleaned_dataset.path, index_col=None) \n",
    "    \n",
    "    #create a train and test dataset\n",
    "    train, test = tts(alldata, test_size=0.3)\n",
    "\n",
    "    #create a train.csv file and a test.csv file\n",
    "    train.to_csv(dataset_train.path + \".csv\" , index=False, encoding='utf-8-sig')\n",
    "    test.to_csv(dataset_test.path + \".csv\" , index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component: Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\"pandas\", \"scikit-learn\"],\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def undersample_train(dataset_train: Input[Dataset], resampled_dataset_train: Output[Dataset]):\n",
    "    '''Apply undersampling to training set'''\n",
    "    import pandas as pd\n",
    "    import logging \n",
    "    import sys\n",
    "    from imblearn.under_sampling import RandomUnderSampler\n",
    "    \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO) \n",
    "    \n",
    "    # get the train dataset from the previous component\n",
    "    train_data = pd.read_csv(dataset_train.path, index_col=None)\n",
    "\n",
    "    # random sampling\n",
    "    rus = RandomUnderSampler(random_state=42)\n",
    "    resampled_train_data = rus.fit_resample(train_data)\n",
    "\n",
    "    # create a file with the randomly undersampled data\n",
    "    resampled_train_data.to_csv(resampled_train_data.path + \".csv\" , index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component: upload the dataset to Google Cloud!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\"google-cloud-storage\"],\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def upload_dataset_to_gcs(project_id: str, resampled_train_data: Input[Dataset], dataset_test: Input[Dataset], assignment: str):\n",
    "    '''upload dataset to gsc'''\n",
    "    from google.cloud import storage   \n",
    "    import logging \n",
    "    import sys\n",
    "    \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)    \n",
    "  \n",
    "    # upload the model to GCS\n",
    "    client = storage.Client(project=project_id)\n",
    "    bucket = client.bucket(bucket)\n",
    "    blob1 = bucket.blob(dataset_test)\n",
    "    blob2 = bucket.blob(resampled_train_data)\n",
    "   \n",
    "    blob1.upload_from_filename(dataset_test.path + \".csv\")   \n",
    "    blob2.upload_from_filename(resampled_train_data.path + \".csv\")\n",
    "    logging.info(\"Upload to Google Cloud complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline: Call all components with the output from the previous component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline component\n",
    "@kfp.dsl.pipeline(\n",
    "    name=\"fraud-predictor-data-cleaning-pipeline\")\n",
    "def pipeline(project_id: str, data_bucket: str, model_repo: str, dataset_feature_name: str, dataset_label_name: str):    \n",
    "    \n",
    "    di_op = download_data(\n",
    "        project_id=project_id,\n",
    "        bucket=data_bucket,\n",
    "        feature_dataset_name= dataset_feature_name,\n",
    "        label_dataset_name = dataset_label_name\n",
    "    )\n",
    "     \n",
    "    clean_op = clean_data(merged_dataset =  di_op.outputs[\"merged_dataset\"]) \n",
    "                                            # cleaned_dataset: Output[Dataset]) \n",
    "                                            \n",
    "    train_test_split_op = train_test_split(cleaned_dataset = clean_op.outputs[\"cleaned_dataset\"])\n",
    "                                           # dataset_train: Output[Dataset], dataset_test: Output[Dataset])\n",
    "    \n",
    "    resampled_train_op = undersample_train(dataset_train = train_test_split_op.outputs[\"dataset_train\"])\n",
    "                                            # resampled_dataset_train: Output[Dataset]\n",
    "\n",
    "    upload_dataset_to_gcs(resampled_train_data = resampled_train_op.outputs[\"resampled_train_op\"], dataset_test = train_test_split_op.outputs[\"dataset_test\"])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import compiler\n",
    "compiler.Compiler().compile(pipeline_func=pipeline,\n",
    "        package_path='assignment1_pipeline.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aip\n",
    "\n",
    "# Before initializing, make sure to set the GOOGLE_APPLICATION_CREDENTIALS\n",
    "# environment variable to the path of your service account.\n",
    "aip.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    ")\n",
    "\n",
    "# Prepare the pipeline job\n",
    "job = aip.PipelineJob(\n",
    "    display_name=\"assignment_1\",\n",
    "    enable_caching=False,\n",
    "    template_path=\"assignment1_pipeline.yaml\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    location=REGION,\n",
    "    parameter_values={\n",
    "        'project_id': PROJECT_ID, # makesure to use your project id \n",
    "        'data_bucket': 'data_de2023_2056332',  # the data_bucket name\n",
    "        'model_repo':'models_de2023_2056332', # GCP model repo bucket name\n",
    "        'dataset_feature_name' : 'Credit_card', # the feature dataset name\n",
    "        'dataset_label_name': 'Credit_card_label' # the label dataset name\n",
    "    }\n",
    ")\n",
    "\n",
    "job.run()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
